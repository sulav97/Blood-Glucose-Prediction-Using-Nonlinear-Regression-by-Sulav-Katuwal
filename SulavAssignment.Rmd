---
title: "Blood Glucose Prediction Using Nonlinear Regression"
author: "Sulav Katuwal"
output:
  pdf_document: default
  html_document: default
---

## Task 1: Preliminary Data Analysis

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)
library(corrplot)
library(scales)
library(gridExtra)
library(MASS)
```


## Data Loading and Overview
```{r load-data}
data <- read.csv("C:/Users/sulav/Downloads/dataset_(MSC7).csv")

cat("EDA: Exploratory Data Analysis - dataset summary\n")
cat("Variables:\n")
print(colnames(data))

cat("First 6 rows:\n")
print(head(data))

cat("Summary of numeric variables:\n")
print(summary(select_if(data, is.numeric)))
```
```{r check-columns}
colnames(data)
```
.

## Data Cleaning and Preprocessing

```{r data-cleaning}
# 1) hr_mean: fill NA with mean
if("hr_mean" %in% names(data)){
  
  # Fill missing hr_mean with mean 
  hr_mean_avg <- mean(data$hr_mean, na.rm = TRUE)
  data$hr_mean[is.na(data$hr_mean)] <- hr_mean_avg
}  


# Print NA summary
cat("Missing values per column:\n")
print(colSums(is.na(data)))

```

.

## Variable Scaling and Normalization

```{r scaling}
scaled_data <- data %>%
  mutate(
    steps_sum_scaled = if("steps_sum" %in% names(data)) rescale(steps_sum) else NA,
    cals_sum_scaled  = if("cals_sum" %in% names(data)) rescale(cals_sum)  else NA,
    insulin_sum_scaled = if("insulin_sum" %in% names(data)) rescale(insulin_sum) else NA
  )

head(scaled_data)
```

.

## 1.1 Time Series Analysis of Input and Output Variables

```{r time-series, fig.height=10}
data$index <- 1:nrow(data)

p1 <- ggplot(data, aes(index, bg_mean)) + geom_line() + ggtitle("bg_mean")
p2 <- ggplot(data, aes(index, insulin_sum)) + geom_line() + ggtitle("insulin_sum")
p3 <- ggplot(data, aes(index, carbs_sum)) + geom_line() + ggtitle("carbs_sum")
p4 <- ggplot(data, aes(index, hr_mean)) + geom_line() + ggtitle("hr_mean")
p5 <- ggplot(data, aes(index, steps_sum)) + geom_line() + ggtitle("steps_sum")
p6 <- ggplot(data, aes(index, cals_sum)) + geom_line() + ggtitle("cals_sum")
p7 <- ggplot(data, aes(index, bg.1.00)) + geom_line(color="red") + ggtitle("Output: bg+1:00")

grid.arrange(p1,p2,p3,p4,p5,p6,p7, ncol=2)
```

.

## 1.2 Distribution Analysis (Histograms and Density Plots)

```{r histograms, fig.height=12}
vars <- c("bg_mean","insulin_sum","carbs_sum","hr_mean","steps_sum","cals_sum","bg.1.00")
present <- vars[vars %in% names(data)]

plot_hist <- function(v){
  ggplot(data, aes_string(v)) +
    geom_histogram(aes(y=..density..), bins=40, fill="lightblue") +
    geom_density(color="red") +
    ggtitle(paste("Histogram & Density:", v))
}

plots <- lapply(present, plot_hist)
do.call(grid.arrange, c(plots, ncol=2))
```


.

##  1.3 Correlation and Scatter Plot Analysis

```{r correlation}
num_vars <- c("bg_mean","insulin_sum","carbs_sum","hr_mean","steps_sum","cals_sum","bg.1.00")
numeric_cols <- num_vars[num_vars %in% colnames(data)]

cor_matrix <- cor(data[,numeric_cols], use="pairwise.complete.obs")

corrplot(cor_matrix, method="color", addCoef.col="black")

# Report high correlation
high_pairs <- which(abs(cor_matrix) > 0.85 & abs(cor_matrix) < 1, arr.ind=TRUE)
if(nrow(high_pairs) == 0){
  cat("No correlation above |0.85|. No columns removed.\n")
} else {
  cat("Highly correlated variable pairs found:\n")
  print(high_pairs)
}
```

## - Rename Output Variable-
```{r rename-output}
data <- data %>% rename(y = bg.1.00)
colnames(data)
```
```{r scatter-panel-a, fig.width=10, fig.height=8}
library(ggplot2)
library(gridExtra)

# Common scatter plot function
scatter_y <- function(xvar, xlabel) {
  ggplot(data, aes_string(x = xvar, y = "y")) +
    geom_point(alpha = 0.3, color = "steelblue") +
    geom_smooth(method = "loess", color = "darkred", se = TRUE) +
    labs(
      title = paste("Scatter Plot:", xlabel, "vs Blood Glucose (bg+1:00)"),
      x = xlabel,
      y = "Blood Glucose One Hour Ahead (mmol/L)"
    ) +
    theme_minimal(base_size = 11)
}

# Individual plots
p1 <- scatter_y("bg_mean", "Mean Blood Glucose")
p2 <- scatter_y("insulin_sum", "Total Insulin Dose")
p3 <- scatter_y("carbs_sum", "Total Carbohydrate Intake")
p4 <- scatter_y("hr_mean", "Mean Heart Rate")
p5 <- scatter_y("steps_sum", "Total Steps")
p6 <- scatter_y("cals_sum", "Total Calories Burned")

# Arrange in 3x2 grid
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)
```

```{r scatter-panel-b, fig.width=12, fig.height=4}
# ========================================
# Predictor-Predictor Relationships
# ========================================

# Predictor-predictor scatter function
scatter_xx <- function(xvar, yvar, xlabel, ylabel) {
  ggplot(data, aes_string(x = xvar, y = yvar)) +
    geom_point(alpha = 0.3, color = "darkgreen") +
    geom_smooth(method = "lm", color = "black", se = FALSE) +
    labs(
      title = paste(xlabel, "vs", ylabel),
      x = xlabel,
      y = ylabel
    ) +
    theme_minimal(base_size = 11)
}

q1 <- scatter_xx("insulin_sum", "carbs_sum",
                 "Total Insulin Dose", "Total Carbohydrate Intake")
q2 <- scatter_xx("steps_sum", "cals_sum",
                 "Total Steps", "Total Calories Burned")
q3 <- scatter_xx("bg_mean", "insulin_sum",
                 "Mean Blood Glucose", "Total Insulin Dose")

grid.arrange(q1, q2, q3, ncol = 3)
```



## Task 2: Regression Modelling

# 2.1 – Estimation of Model Parameters Using Least Squares
```{r fit-models}
model1 <- lm(y ~ I(bg_mean^3) + I(insulin_sum^2) + I(carbs_sum^2) + hr_mean + steps_sum + cals_sum, data=data)
model2 <- lm(y ~ I(bg_mean^2) + I(insulin_sum^2) + I(carbs_sum^3) + hr_mean + steps_sum + cals_sum, data=data)
model3 <- lm(y ~ bg_mean + insulin_sum + carbs_sum + I(hr_mean^2) + steps_sum + I(cals_sum^2), data=data)
model4 <- lm(y ~ I(bg_mean^2) + I(insulin_sum^2) + I(carbs_sum^2) + I(hr_mean^2) + I(steps_sum^2) + I(cals_sum^2), data=data)
model5 <- lm(y ~ bg_mean + insulin_sum + carbs_sum + hr_mean + steps_sum + cals_sum +
               bg_mean:insulin_sum + carbs_sum:hr_mean + insulin_sum:cals_sum, data=data)

models_list <- list(model1=model1, model2=model2, model3=model3, model4=model4, model5=model5)

lapply(models_list, coef)
```

.

##  2.2 – Computation of Residual Sum of Squares (RSS)

##  2.3 – Computation of Log-Likelihood Functions

##  2.4 – Computation of AIC and BIC


```{r model-metrics}
metrics <- data.frame(
  Model = names(models_list),
  RSS = sapply(models_list, function(m) sum(residuals(m)^2)),
  logLik = sapply(models_list, function(m) as.numeric(logLik(m))),
  AIC = sapply(models_list, AIC),
  BIC = sapply(models_list, BIC)
)

metrics
```

#  Combined 3 panel-

```{r Combined 3-Panel}
## - Combined 3-Panel Grid: RSS, AIC, BIC Comparison

library(gridExtra)

p_rss <- ggplot(metrics, aes(x = Model, y = RSS)) +
  geom_bar(stat = "identity") +
  ggtitle("RSS Comparison Across Models") +
  ylab("RSS") +
  theme_minimal()

p_aic <- ggplot(metrics, aes(x = Model, y = AIC)) +
  geom_bar(stat = "identity") +
  ggtitle("AIC Comparison Across Models") +
  ylab("AIC") +
  theme_minimal()

p_bic <- ggplot(metrics, aes(x = Model, y = BIC)) +
  geom_bar(stat = "identity") +
  ggtitle("BIC Comparison Across Models") +
  ylab("BIC") +
  theme_minimal()

grid.arrange(p_rss, p_aic, p_bic, ncol = 3)
```


## 5×2 Cross-Validation (Model Performance Evaluation)

```{r 5x2 Cross-Validation}
## - Task 2 – 5×2 Cross-Validation (shuffled evaluation)

set.seed(999)

# Store formulas
formulas <- list(
  model1 = y ~ I(bg_mean^3) + I(insulin_sum^2) + I(carbs_sum^2) + hr_mean + steps_sum + cals_sum,
  model2 = y ~ I(bg_mean^2) + I(insulin_sum^2) + I(carbs_sum^3) + hr_mean + steps_sum + cals_sum,
  model3 = y ~ bg_mean + insulin_sum + carbs_sum + I(hr_mean^2) + steps_sum + I(cals_sum^2),
  model4 = y ~ I(bg_mean^2) + I(insulin_sum^2) + I(carbs_sum^2) + I(hr_mean^2) + I(steps_sum^2) + I(cals_sum^2),
  model5 = y ~ bg_mean + insulin_sum + carbs_sum + hr_mean + steps_sum + cals_sum +
           bg_mean:insulin_sum + carbs_sum:hr_mean + insulin_sum:cals_sum
)

cv_results <- data.frame()

for (s in 1:5) {
  shuffled_data <- data[sample(nrow(data)), ]
  fold_size <- floor(nrow(shuffled_data) / 2)
  fold1 <- 1:fold_size
  fold2 <- (fold_size+1):nrow(shuffled_data)
  
  folds <- list(fold1, fold2)
  
  for (f in 1:2) {
    test_index <- folds[[f]]
    train_index <- setdiff(1:nrow(shuffled_data), test_index)
    
    train <- shuffled_data[train_index, ]
    test  <- shuffled_data[test_index, ]
    
    for (m in names(formulas)) {
      fit <- lm(formulas[[m]], data=train)
      preds <- predict(fit, newdata=test)
      
      rss  <- sum((test$y - preds)^2)
      rmse <- sqrt(mean((test$y - preds)^2))
      aic  <- AIC(fit)
      bic  <- BIC(fit)
      
      cv_results <- rbind(cv_results,
        data.frame(
          shuffle=s,
          fold=f,
          model=m,
          RSS=rss,
          RMSE=rmse,
          AIC=aic,
          BIC=bic
        )
      )
    }
  }
}

cv_summary <- aggregate(
  cbind(RSS, RMSE, AIC, BIC) ~ model,
  data=cv_results,
  FUN=mean
)

print(cv_summary)
```


Combined bar plots removed because RSS, AIC, and BIC operate on different scales.
Instead, separate plots (RSS, AIC, BIC) are provided for valid comparison.


## 2.5 Residual Diagnostics and Normality Assessment

```{r residuals, fig.height=12}
plot_res <- function(model, name){
  r <- residuals(model)
  df <- data.frame(resid = r)

  p1 <- ggplot(df, aes(x = resid)) +
    geom_histogram(aes(y = after_stat(density)), bins = 40) +
    geom_density(color = "red") +
    ggtitle(paste("Residual Histogram:", name))

  p2 <- ggplot(df, aes(sample = resid)) +
    stat_qq() +
    stat_qq_line(color = "red") +
    ggtitle(paste("Q-Q Plot:", name))

  grid.arrange(p1, p2, ncol = 2)
}

lapply(names(models_list), function(n) plot_res(models_list[[n]], n))
```



## 2.6 Selection of the Best Regression Model

```{r best-model}
best_name <- metrics$Model[which.min(metrics$AIC)]
best_model <- models_list[[best_name]]

cat("Best model (dataset-specific):", best_name)
best_model
```

.

## 2.7 – Train/Test Split, Prediction, and 95% Confidence Intervals

```{r train-test, fig.height=7}
set.seed(123)

n <- nrow(data)
train_idx <- sample(1:n, size = floor(0.7 * n))   # 70% training
train_data <- data[train_idx, ]
test_data  <- data[-train_idx, ]

best_formula <- formula(best_model)
best_model_train <- lm(best_formula, data=train_data)

# Prediction with standard errors
pred <- predict(best_model_train, newdata=test_data, se.fit = TRUE)
crit <- qt(0.975, df = nrow(train_data) - length(coef(best_model_train)))

pred_df <- data.frame(
  index = 1:nrow(test_data),
  actual = test_data$y,
  fitted = pred$fit,
  lower  = pred$fit - crit * pred$se.fit,
  upper  = pred$fit + crit * pred$se.fit
)

ggplot(pred_df, aes(index)) +
  geom_line(aes(y = actual), color = "black") +
  geom_line(aes(y = fitted), color = "blue") +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +
  ggtitle("Test Predictions with 95% CI (70/30 Split)")
```

```{r test-metrics}
RMSE <- sqrt(mean((pred_df$actual - pred_df$fitted)^2))
MAE  <- mean(abs(pred_df$actual - pred_df$fitted))

cat("Test RMSE (70/30) =", RMSE, "\n")
cat("Test MAE  (70/30) =", MAE, "\n")
```

##Task 3: Approximate Bayesian Computation (ABC)

## 3.1 – Selection of the Two Most Influential Model Parameters

```{r abc-setup}
y_vec <- data$y
mf <- model.frame(best_formula, data=data)
Xmat <- model.matrix(best_formula, data=mf)

theta_hat <- coef(best_model)
coef_no_int <- theta_hat[names(theta_hat)!="(Intercept)"]

top2 <- names(sort(abs(coef_no_int), decreasing=TRUE))[1:2]
top2
```

.

## 3.2 – Prior Distribution Specification

```{r abc-priors}
r <- 0.5
idx <- match(top2, names(theta_hat))

theta1_hat <- theta_hat[idx[1]]
theta2_hat <- theta_hat[idx[2]]

prior1 <- c(theta1_hat*(1-r), theta1_hat*(1+r))
prior2 <- c(theta2_hat*(1-r), theta2_hat*(1+r))

prior1; prior2
```

.

## 3.3 – Rejection ABC Sampling Procedure

```{r abc-rejection, message=FALSE}
set.seed(123)
N <- 20000

theta1_s <- runif(N, prior1[1], prior1[2])
theta2_s <- runif(N, prior2[1], prior2[2])

p <- length(theta_hat)
theta_mat <- matrix(rep(theta_hat, each=N), ncol=p, byrow=FALSE)
colnames(theta_mat) <- names(theta_hat)

theta_mat[, idx[1]] <- theta1_s
theta_mat[, idx[2]] <- theta2_s

rss_vec <- numeric(N)
for(i in 1:N){
  rss_vec[i] <- sum((y_vec - Xmat %*% theta_mat[i, ])^2)
}

eps <- quantile(rss_vec, 0.01)
accepted <- which(rss_vec <= eps)

length(accepted)
```

.

## 3.4 – Joint and Marginal Posterior Distributions

```{r abc-plots, fig.height=8}
post1 <- theta_mat[accepted, idx[1]]
post2 <- theta_mat[accepted, idx[2]]

p1 <- ggplot(data.frame(post1), aes(post1)) +
  geom_histogram(aes(y=..density..), bins=30) +
  geom_density(color="red") +
  ggtitle(paste("Posterior:", top2[1]))

p2 <- ggplot(data.frame(post2), aes(post2)) +
  geom_histogram(aes(y=..density..), bins=30) +
  geom_density(color="red") +
  ggtitle(paste("Posterior:", top2[2]))

p_joint <- ggplot(data.frame(post1, post2), aes(post1, post2)) +
  geom_point(alpha=0.5) +
  ggtitle("Joint Posterior")

grid.arrange(p1,p2,p_joint, ncol=2)
```


```{r abc-contour, fig.height=6, fig.width=6}
# Create dataframe
post_df <- data.frame(post1 = post1, post2 = post2)

# 2D Kernel Density Estimation
kde <- kde2d(post_df$post1, post_df$post2, n = 100)

# Convert KDE output to dataframe
kde_df <- with(kde, expand.grid(x = x, y = y))
kde_df$z <- as.vector(kde$z)

# Contour plot
ggplot(kde_df, aes(x = x, y = y, z = z)) +
  geom_contour_filled(alpha = 0.8) +
  geom_point(
    data = post_df,
    aes(x = post1, y = post2),
    inherit.aes = FALSE,   # ✅ THIS FIXES THE ERROR
    color = "black",
    alpha = 0.2,
    size = 0.4
  ) +
  labs(
    title = "Joint Posterior Distribution (Contour Plot)",
    x = top2[1],
    y = top2[2],
    fill = "Posterior Density"
  ) +
  theme_minimal()
```

## 3.5 – Posterior Summary and Interpretation

```{r abc-summary}
summary_df <- data.frame(
  parameter = top2,
  mean = c(mean(post1), mean(post2)),
  median = c(median(post1), median(post2)),
  sd = c(sd(post1), sd(post2))
)

summary_df
```



